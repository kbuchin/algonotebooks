{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do-it-yourself guide to algorithm analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this notebook:** When introducing a concept in class, we typically look at the general theory and at examples. Looking back, it is not always easy to see how the step from the theory to the examples is achieved. The aim of this notebook is to show how to fill in these steps.\n",
    "\n",
    "**Note:** In the guides, click on $\\triangleright$ to expand an explanation/example/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "### 1. General Analysis\n",
    "\n",
    "* [Big-O: Deriving simple bounds](#bigO-intuitive)\n",
    "* [Big-O: Prove $f(n) = O(g(n))$ (likewise $\\Omega(g(n))$, $\\Theta(g(n))$)](#bigO-positive)\n",
    "* [Big-O: Prove $f(n) \\ne O(g(n))$ (likewise $\\Omega(g(n))$, $\\Theta(g(n))$)](#bigO-negative)\n",
    "* [Simple Induction](#induction)\n",
    "* [Basic proof techniques](#proofs)\n",
    "* [Sums](#sums)\n",
    "\n",
    "### 2. Running Time Analysis\n",
    "\n",
    "* [Deriving the running time](#rt-derive)\n",
    "* [Analyzing loops; sums](#rt-loops)\n",
    "* [Recurrences: Master theorem](#rt-mastertheorem)\n",
    "* [Recurrences: Recursion tree](#rt-recursiontree)\n",
    "* [Recurrences: Substitution method](#rt-substitution)\n",
    "* [Amortized Analysis](#rt-amortized)\n",
    "\n",
    "### 3. Proving Correctness\n",
    "\n",
    "* [Loops: Loop invariant proofs](#correctness-loops)\n",
    "* [Recursion: Strong Induction](#correctness-recursion)\n",
    "* [Using basic algorithms and data structures](#correctness-additional)\n",
    "\n",
    "### 4. Executing Algorithms\n",
    "\n",
    "* [Hashtables: chaining](#alg-chaining)\n",
    "* [Hashtables: open addressing/probing](#alg-probing)\n",
    "* [Heaps: Heapify/Heapsort](#alg-heaps)\n",
    "* [Binary search trees: Successor/Predecessor](#alg-successor)\n",
    "* [Graphs: BFS](#alg-bfs)\n",
    "* [Graphs: DFS/topological ordering](#alg-dfs)\n",
    "* [Graphs: Dijkstra's algorithm](#alg-dijkstra)\n",
    "\n",
    "### 5. Algorithm Design\n",
    "\n",
    "* [Incremental algorithms](#design-incremental)\n",
    "* [Divide&Conquer](#design-divide)\n",
    "* [Using basic algorithms and data structures](#design-additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O: Deriving simple bounds <a class=\"anchor\" id=\"bigO-intuitive\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Determining the growth rate.*\n",
    "\n",
    "In short: determine the dominant term.\n",
    "\n",
    "1. If the function/expression is a sum, then determine the term which for very large $n$ will be largest. If you like to think of this as limits, $f(n)$ has a larger growth rate than $g(n)$ if $f(n)/g(n)$ goes to infinity if $n$ goes to infinity. This is the dominant term. Take into account:  \n",
    " (a) Exponential functions grow faster than polynomial functions,  \n",
    " (b) polynomial functions grow faster than logarithmic functions\n",
    " \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ examples\n",
    "  </summary>\n",
    "  <p>\n",
    "    (a) The growth rate of $2^n + n^{10}$ is $\\Theta(2^n)$    \n",
    "    (b) The growth rate of $n + \\log n$ is $\\Theta(n)$  \n",
    "    (c) The growth rate of $n \\log n$ is $\\Theta(n \\log n)$ (and **not** $\\Theta(n)$, since here we don't have a sum but a product)\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "2. Remove any constant factors from the dominant term. The result is what we call the *growth rate* of the function, or more precisely, we denote the growth rate as $\\Theta(\\ldots)$ of this term\n",
    "\n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example\n",
    "  </summary>\n",
    "  <p>\n",
    "    The growth rate of $20 n^2 - 5n$ is $\\Theta(n^2)$\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "*Comparing growth rates.*\n",
    "\n",
    "We already compared growth rates above, when determining the dominant term. Here I would only like to point out, that you should be careful with the notation, $O(\\ldots), \\Omega(\\ldots)$, and $\\Theta(\\ldots)$. Since we don't use $\\Omega$ that often, it is sometimes forgotten that $\\Omega$ denotes a lower bound on the growth rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O: Prove $f(n) = O(g(n))$ (likewise $\\Omega(g(n))$, $\\Theta(g(n)))$ <a class=\"anchor\" id=\"bigO-positive\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proving $f(n) = O(g(n))$**  \n",
    "\n",
    "Assume we want to prove that $5n = O(4n-10)$. We need to find $c>0$ and $n_0$ such that $5n \\leq c (4n -10)$ for all $n \\ge n_0$. \n",
    "\n",
    "Now, if we would need to prove that $5n = O(4n)$ then that would be easier, since we could simply choose $c = \\frac{5}{4}$. We would immediately get $5n \\le c 4n$, since actually $5n = c 4n$. But we will need to deal with the '-10' on the right hand side. We will do this, by picking a suitable $n_0$ such that '-10' is small compared to $4n$. So now lets actually prove the original statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof:* If we choose $n_0 = 10$, then for $n \\ge n_0$ we have $4n - 10 = (3n + n) - 10 = 3n + (n - 10) \\ge 3n$. Now choose $c = \\frac{5}{3}$. We get $0 < 5n = c 3n \\le 4n - 10$ for all $n \\ge n_0$, which is what we had to prove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, when proving $f(n) = O(g(n))$, we do the following:\n",
    "\n",
    "1. If $f(n)$ and $g(n)$ have no lower-order terms, pick a $c > 0$ for which you can easily prove $f(n) \\leq c g(n)$.\n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ comments/example\n",
    "  </summary>\n",
    "  <p>\n",
    "    There is no need for choosing $c$ as small as possible. If we for instance want to prove $5n - 5 = O(n^3)$, then simply choose $c=5$ even if a smaller $c$ would work. You also need to pick $n_0$, but if there is no particular difficulty for small $n$, simply pick $n_0 = 1$. For expressions involving $\\log n$ you may need to pick $n_0 = 2$.\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "\n",
    "2. If $f(n)$ has negative lower-order terms, you can drop them, since you are proving an inquality with $\\leq$. Likewise if $g(n)$ has positive terms, these can be easily handled by an inequality. \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example\n",
    "  </summary>\n",
    "  <p>\n",
    "For instance, to prove $5n -5 = O(n^3 + n^2)$, it is enough to look at $5n$ and $n^3$, and pick $c = 5$ (see step 1). Now, $5n - 5 \\leq 5n \\leq c n^3 \\leq c (n^3 + n^2)$.\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "\n",
    "3. If $f(n)$ has positive lower order terms, each of these can be bounded as in step 1. This will give several $c$s which we can then add up. \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example\n",
    "  </summary>\n",
    "  <p>\n",
    "For instance, if we want to prove $5n + 11 = O(n^3)$, we can make use of $5n \\leq 5 n^3$ (Thus, the first $c$ is 5) and $11 \\leq 11 n^3$ (Thus, the second $c$ is 11). Here, we can overall pick $c = 5 + 11 = 16$, to prove $5n + 11 \\leq 5n^3 + 11n^3 = 16n^3 = cn^3$.\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "\n",
    "4. If $g(n)$ has lower order terms, more care is needed. Here we often need to pick a larger $n_0$. Specifically, we pick $n_0$ such that the lower order terms can be bounded in terms of a fraction (e.g., 1/2) of the dominant term. Then we can proceed as in step 1 (after possibly first using step 2+3 to get rid of terms that can be removed easily), but need to take into account that this fraction is missing. \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example\n",
    "  </summary>\n",
    "  <p>\n",
    "For instance, in the example above, we wanted to prove $5n = O(4n - 10)$. The lower-order term is $10$, the dominant term $4n$. If we want, that the lower order term is a fraction (lets indeed say 1/4) of the dominant term, we first pick $n_0$ such that for $n \\geq n_0$  $10 \\leq 1/4 \\cdot 4n = n$. This holds starting with $n_0 = 10$. Now if we pick $n_0=10$, we now do step 1, but we will want to pick $c$ taking into account the fraction that we removed, that is, we pick $c$ such that $5n \\leq c \\cdot (1-1/4)\\cdot 4n = c\\cdot 3/4 \\cdot 4n = 3n$. Therefore, with $c = 5/3$ and $n_0 = 10$, we can prove $5n = 5/3 \\cdot 3n = 5/3 (4n - n) \\leq 5/3 (4n-10)$, where the last inequality holds because $n \\geq n_0 = 10$.\n",
    "  </p>\n",
    "</details>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proving $f(n) = \\Omega(g(n))$**  \n",
    "\n",
    "To prove $f(n) = O(g(n))$, we had to find a $c>0$ (and $n_0$, s.t for  $n \\geq n_0$) such that $f(n) \\leq c g(n)$. The only difference now, is a $\\geq$ instead of the $\\leq$, that is, we need to prove $f(n) \\geq c g(n)$. The steps are essentially the same as before. \n",
    "\n",
    "1. If there are no lower-order terms, just find a $c$. \n",
    "\n",
    "2. If there are lower order terms, be carefully which of them can be ignored. Now, a positive lower-order term in $f(n)$ can be handled easily by the inequality, and likewise a negative lower-order term of $g(n)$ \n",
    "\n",
    "3. If $g(n)$ has positive lower-order terms, either (Option A) first find a $c'$ that works for each of the terms separately (as in step 1); now choose $c = c'/k$, or (Option B, preferred) do step 3 of $g(n) = O(f(n))$; this will give you a $c'$ such that $g(n) \\leq c' f(n)$; choose $c = 1/c'$,\n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example (Option A)\n",
    "  </summary>\n",
    "  <p>\n",
    "Assume we want to prove $n = \\Omega (2n + 5)$. We have $n \\geq c' 2n$ and $n \\geq c' 5$ for $c' = 1/5$ (which is the smaller of 1/2 and 1/5). We have 2 terms, so lets pick $c= c'/2 = 1/10$ (and $n_0 =1). Now, $n = 1/10 \\cdot 10n \\geq 1/10 \\cdot 7n =  1/10 \\cdot (2n + 5n) \\geq 1/10 (2n + 5). Note that $c = 1/7$ would have been the somewhat more natural choice; change your proof if you make such an observation.\n",
    "  </p>\n",
    "</details> \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example (Option B)\n",
    "  </summary>\n",
    "  <p>\n",
    "Assume we want to prove $n = \\Omega (2n + 5)$. Assume we would want to prove $2n +5 = O(n)$ (which is actually equivalent. For this we would choose (see Step 3 above) c'=7, and obtain $2n + 5 \\leq 2n + 5n = 7n$. Thus, here we can take $c = 1/c' = 1/7$, and obtain $n = 1/7 \\cdot 7n = 1/7 \\cdot (2n + 5n) \\geq 1/7 (2n + 5)$>\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "4. If $f(n)$ has a negative lower-order term, we need to bound this term (for suitable $n_0$) by the dominant term. We do this in the same way, as we did for $g(n)$ in step 4 of $f(n) = O(g(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proving $f(n) = \\Theta(g(n))$**  \n",
    "\n",
    "$f(n) = \\Theta(g(n))$ is equivalent to $f(n) = O(g(n))$ **and** $f(n) = \\Omega(g(n))$. We need to prove both and we can do this as above. We call the $c$s $c_1$ and $c_2$. To stay close to the definition of $\\Theta()$, we use the same $n_0$ for both. For this we can simply take the larger of the two $n_0$s that we get in the two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O: Prove $f(n) \\ne O(g(n))$ (likewise $\\Omega(g(n))$, $\\Theta(g(n)))$ <a class=\"anchor\" id=\"bigO-negative\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proving $f(n) \\neq O(g(n))$, or $f(n) \\neq \\Omega(g(n))$** \n",
    "\n",
    "The proof technique above is useful if we want to prove a statement of the type $f(n) = O(g(n))$. But what if we want to prove that $f(n) = O(g(n))$ is a false statement? Now if we for instance want to prove that for instance $n^2 = O(n)$ is false, we can use a *proof by contradiction*. This means that we prove that the statement is false by assuming that it is true, and then leading this to a false statement. Then we know that our original assumption must have been already false. \n",
    "\n",
    "Often enough such a proof ends with a statement, that as such is not false, but that contradicts our original assumption. This is just as good to conclude that our orginal assumption must have been wrong; and essentially we also end up with a false statement, because the original statement with the statement that is wrong together form a false statement. Lets try this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof (by contradiction):* Suppose there is a $c>0$ and an $n_0$ such that $n^2 \\le c n$ for all $n \\ge n_0$. This is equivalent to $n \\le c$. But for large enough $n$, we eventually have $n > c$ no matter how large we picked the constant $c$. A contradiction. So $n^2 = O(n)$ is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proving $f(n) \\neq \\Theta(g(n))$** \n",
    "\n",
    "$f(n) = \\Theta(g(n))$ is equivalent to $f(n) = O(g(n))$ **and** $f(n) = \\Omega(g(n))$. Thus, if we want to prove that $f(n) \\neq \\Theta(g(n))$, it is enough to prove either $f(n) \\neq O(g(n))$ **or** $f(n) \\neq \\Omega(g(n))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Induction <a class=\"anchor\" id=\"induction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Make sure to understand why an induction proof actually constitutes a proof of a statement. This makes it much easier to write clear and correct inductions.\n",
    "\n",
    "Step 1: Induction proves essentially always follow the same structure. Make sure to use this structure, see [notes on induction](https://www.win.tue.nl/~kbuchin/teaching/JBP030/notebooks/Induction.html).\n",
    "\n",
    "A common use case for simple induction is proving that a sum has a certain closed form, e.g., that the arithmetic series $\\sum_{i=1}^n i= n(n+1)/2$. In this case the next steps are most commonly as follows:\n",
    "\n",
    "Step 2: Formulate the induction hypothesis. Very often this is simply the statement that you eventually want to prove but without the \"for all\" quantifier at the beginning. I.e., in the example the IH would be $\\sum_{i=1}^n i = n(n+1)/2$, while the statement that we eventually want to prove is that \"for all $n$: $\\sum_{i=1}^n i= n(n+1)/2$\".\n",
    "\n",
    "Step 3: The base case is most commonly simply the value of the first $i$ (e.g., in the example of the arithmetic series, the base case is $n=1$). Check that the left-hand side of the equality that you want to prove, when plugging in the base case as $n$, equals the right-hand side. E.g., $\\sum_{i=1}^1 i = 1$ and $1\\cdot(1+1)/2 =1$.\n",
    "\n",
    "Step 4: In the induction step (going from $n$ to $n+1$) you most likely first split of the last term, e.g., rewrite  $\\sum_{i=1}^{n+1} i = \\sum_{i=1}^{n} i + n +1$. Then you can apply the induction hypothesis (make sure to actually write that you are using it) on the remaining sum, and the rest is simply a matter of rewriting the resulting sum. E.g., $\\sum_{i=1}^{n+1} i = \\sum_{i=1}^{n} i + n +1 =_{IH} n(n+1)/2 + n+1 = (n+1) \\cdot (n/2 +1) = (n+1)(n+2)/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Proof Techniques <a class=\"anchor\" id=\"proofs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When proving a statement, it is useful to be aware of specific types/techniques of proofs. In this course, the most relevant ones are:\n",
    "\n",
    "- Induction: see simple induction [above](#induction), but for us more important is strong induction. We use strong induction to prove correctness of recursive algorithms and in the substitution method for recurrences\n",
    "\n",
    "- Loop Invariant Proofs: In principle, this is a type of induction, but listed here separately, in particular because of their importance in the analysis of algorithms.\n",
    "\n",
    "- Proof by contradiction: A common way to prove a statement is to assume that it does not hold, and then lead this to a contradiction. We use this often when we want to prove a statement of the type \"There exists no constant/algorithm/object $\\ldots$\". An example is the statement $f(n) \\neq O(g(n))$, see [above](#bigO-negative). \n",
    "\n",
    "- Proof by construction: If a statement is of the type \"There exists a constant/algorithm/object $\\ldots$\", giving an example of such a constant/algorithm/object is a valid proof. An example is the statement $f(n) = O(g(n))$, see [above](#bigO-positive). Since this statement means that there is a $c>0$ and an $n_0$ such that $\\ldots$, it is a valid proof to pick a $c$ and an $n_0$ and show that the statement holds with these. Likewise, a statement like \"The sorting problem can be solved in $O(n \\log n)$ time\" is a statement about the existence of a fast algorithm, and therefore can be proven by giving an algorithm. *Careful: Generally an example is not a proof. For instance, to show that the sorting problem can't be solved faster than in $\\Omega(n \\log n)$ time, it is not enough to look at one algorithm and to show that it is not faster.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sums <a class=\"anchor\" id=\"sums\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The techniques below are discussed in the notebook on [mathematical background(https://www.win.tue.nl/~kbuchin/teaching/JBP030/notebooks/maths.html). So here I am only going to list them:\n",
    "\n",
    "1.) Know the notation, i.e., what does $\\sum_{i=1}^n f(i)$ mean.\n",
    "\n",
    "2.) Useful techniques\n",
    "(a) linearity: If there is a factor that does not depend on the index variable, then it can be taken out of the sum\n",
    "\n",
    "(b) splitting/linearity: If there are several summands, you can split the sum into a sum per summand\n",
    "\n",
    "(c) splitting at an intermediate index: Not needed that often, but you can also split the range over which the index runs. A common setting where we do this is simple induction: there we often split off the last summand\n",
    "\n",
    "(d) reversing the sum: In particular if you have a sum of the type $\\sum_{i=0}^{n-1} (n-i)$, you may want to reverse the sum, that is, in this example don't sum it as $(n-0) + (n-1) + \\ldots + 1$, but as $1 + \\ldots + n$.\n",
    "\n",
    "3.) Know the most important sums:\n",
    " (a) arithmetic sum: It is good to know what it exactly evaluates to, but for us the growth rate is mostly sufficient, i.e., $\\sum_{i=1}^n i = \\Theta(n^2)$.\n",
    " \n",
    " (b) geometric sum ($\\sum_{i=0}^n a^i$): Again, know the growth rate. This depends on whether the base $a$ is $<1$ or $>1$. For $a<1$,  $\\sum_{i=0}^n a^i = \\Theta(1)$. For $a>1$, $\\sum_{i=0}^n a^i = \\Theta(a^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the running time <a class=\"anchor\" id=\"rt-derive\"></a>\n",
    "\n",
    "*Step 1:* Number the lines. If you are analyzing the algorithm on paper, or are using pseudo-code, I would suggest to write actual numbers next to the lines of code. If you have code in a Jupyter notebook, you can display line numbers by going to command mode (CTRL - M (or ESC)) and then pressing L. These line numbers will not display when you export the notebook to html or pdf, but for the purpose of this course, you can assume that the reader can count line numbers.\n",
    "\n",
    "*Step 2:* Go through the code line by line.   \n",
    " (a) First mark all lines with only constant-time operations with $\\Theta(1)$\n",
    " (b) Loops: If all of the lines within the loop (i.e., the body of the loop) are already marked with a running time, then compute the overall running time of the loop. There are two cases: the simpler case is that the running time of the body of the loop does not depend on the index variable. Then the running time is simply: number of iterations $\\times$ running time of body. If the running time of the body depends on the index variable, then you will need to explicitely compute the sum over all iterations, see [below](#rt-loops).\n",
    " \n",
    " (c) Recursive calls: These you typically handle last in the derivation of the running time. \n",
    " \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ detailed comments\n",
    "  </summary>\n",
    "  <p>\n",
    " Denote the worst-case running time of the algorithm on an input of size $n$ by $T(n)$. Now each recursive call will result in an $T(\\ldots)$ term. What to fill in between the brackets, depends on the size of the subproblem the algorithm recurses on. You will typically assume that your original input size is $n$. If for instance the algorithm recurses on a subproblem of half the size, then the term you obtain is $T(n/2)$. If the algorithm recurses twice on subproblems of half the size (e.g., MergeSort), then you get two terms of the type $T(n/2)$, i.e., you get $T(n/2) + T(n/2) = 2 T(n/2)$. Careful: If there is an If-else-statement with recursive calls, check whether the recursive calls can actually occur both at the same time. E.g., the code of binary search has two recursive calls, but only one is ever executed. Therefore the binary search recurrence only has one term $T(n/2)$.\n",
    "  </p>\n",
    "</details> \n",
    "\n",
    " \n",
    " (d) Base case: If your algorithm is recursive, it also should have a base case. Make sure to take it into account in your analysis.\n",
    " \n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ detailed comments\n",
    "  </summary>\n",
    "  <p>\n",
    "Sometimes this case is implicit, e.g., the algorithm might start with \"If $n>1$:\", in which case quite likely $n=1$ is the base case, and the algorithm simply does nothing in the base case. In any way, if you are analyzing a recursive algorithm, identify the base case and include it in your analysis. You should also include it when you write down the recurrence. E.g., \n",
    "    \n",
    "    $T(n) = \\cases{2T(n/2) + \\Theta(n) & \\text{if } n > 1\\\\\n",
    "                   \\Theta(1)           & \\text{if } n = 1}$.\n",
    "                   \n",
    "Note: If you would want to be precise, you may also include whether certain terms are rounded up or down, e.g., $2T(n/2)$ might be short for $T(\\lfloor n/2 \\rfloor) + T(\\lceil n/2 \\rceil)$, but you can leave this out, since it doesn't influence the analysis. And I'd recommend to leave it out, because then you also avoid rounding incorrectly.\n",
    "  </p>\n",
    "</details> \n",
    " \n",
    "\n",
    "    \n",
    "### Analyzing loops <a class=\"anchor\" id=\"rt-loops\"></a>\n",
    "\n",
    "As starting point consider the loops in counting sort, selection sort and insertion sort, see [sorting](https://www.win.tue.nl/~kbuchin/teaching/JBP030/notebooks/sorting.html).\n",
    "\n",
    "*Cases for for-loops:*\n",
    "\n",
    "*Case 1: Running time per iteration does not depend on index.* \n",
    "\n",
    "In counting sort the loops are of the form.\n",
    "\n",
    "```python\n",
    "for i in range(1,k+1):\n",
    "    C[i] += C[i-1]\n",
    "```\n",
    "\n",
    "The body of the loop takes constant time ($\\Theta(1)$). Therefore we just need to multiply this with the number of iterations ($k$) to get the overall running time of this loop: $\\Theta(k)$.\n",
    "\n",
    "*Case 2: Running time per iteration is a function in the index variable.* \n",
    "\n",
    "In selection sort the loops are\n",
    "\n",
    "```python\n",
    "for i in range(n):\n",
    "    smallest = i\n",
    "    for j in range(i+1 , n):\n",
    "        if A[j] < A[smallest]: smallest = j\n",
    "    swap(A, smallest, i )\n",
    "```\n",
    "  \n",
    "Line 4 takes $\\Theta(1)$ time, so for the inner loop we are in the first case. The number of iterations is $n - (i+1) = n - i - 1$. Thus, the running time of the inner loop is $\\Theta(n-i-1)$.\n",
    "\n",
    "The body of the outer loop has two additional line that take $\\Theta(1)$ time. Thus the time per iteration is $\\Theta(1) + \\Theta(n-i-1)$, which is a function in $i$.\n",
    "\n",
    "In such a case simply take the sum over all iterations: $\\sum_{i=0}^{n-1} (\\Theta(1) + \\Theta(n-i-1))$. After reversing the sum, we essentially have the arithmetic sum (see [sums](#sums)), resulting in $\\Theta(n^2)$.\n",
    "\n",
    "\n",
    "*Additional cases for while-loops:*\n",
    "\n",
    "*Case 3: Number  iteration varies but you have a worst-case bound, and the worst case can actually be reached each time the loop is executed.*\n",
    "\n",
    "This is for instance the case for insertion sort. It uses a while-loop (the inner loop), but the number of iterations, in the worst-case is always $i$. Here you can use *Case 2* above.\n",
    "\n",
    "*Case 4: Running time per iteration varies, but on average is lower than the worst case.*\n",
    "\n",
    "Here you want to use [amortized analysis](rt-amortized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrences: Master theorem <a class=\"anchor\" id=\"rt-mastertheorem\"></a>\n",
    "\n",
    "To apply the Master Theorem to a recurrence $T(n) = aT(n/b) + f(n)$, you can go through the following sequence of steps:\n",
    "\n",
    "1. Note down: $a = \\ldots$, $b = \\ldots$, $f(n) = \\ldots$. \n",
    "\n",
    "2. Compute/estimate $\\log_b(a)$\n",
    "\n",
    "3. Compare the asymptotic growth of $f(n)$ and $n^{\\log_b(a)}$\n",
    "\n",
    " (a) if $f(n)$ grows slower than $n^{\\log_b(a)}$: go to Case 1\n",
    " (b) if $f(n)$ and $n^{\\log_b(a)}$ have the same growth rate: go to: Case 2\n",
    " (c) if $f(n)$ grows faster than $n^{\\log_b(a)}$: go to Case 3\n",
    " \n",
    "Case 1: Determine $\\varepsilon > 0$ such that $f(n) = O(n^{\\log_b(a)-\\varepsilon})$. If such an  $\\varepsilon$ exists, case 1 of the Master theorem applies, and you can conclude $T(n) = \\Theta (n^{\\log_b(a)})$. If no such $\\varepsilon$ exists, you cannot apply the Master theorem.\n",
    "\n",
    "Case 2: Since $f(n) = \\Theta(n^{\\log_b(a)})$, case 2 of the Master theorem applies, and you can conclude $T(n) = \\Theta(n^{\\log_b(a)} \\log n)$.\n",
    "\n",
    "Case 3: Determine $\\varepsilon > 0$ such that $f(n) = \\Omega(n^{\\log_b(a)+\\varepsilon})$. If such an $\\varepsilon$ exists, you next still need to check the *regularity condition*: Compute $a f(n/b)$. If it equals (or smaller equal to) $c f(n)$ for a $c <1$, then that is the $c$ needed. In this case Master theorem, case 3, applies and you can conclude $T(n) = \\Theta (f(n))$. If you can't find a $\\varepsilon$ or can't find a $c$, then you can't use the Master theorem\n",
    "\n",
    "### Recurrences: Recursion tree <a class=\"anchor\" id=\"rt-recursiontree\"></a>\n",
    "\n",
    "While recursion trees can handle more general recurrences, we focus on recurrences of the type $T(n) = a T(n/b) + f(n)$. For the purpose of drawing a recursion trees, you should read \"$a T(n/b)$\" as \"$T(n/b) + \\ldots + T(n/b)$\" with $a$ summands. Now, the idea of a recursion tree is that you can compute $T(n)$ by adding up $f(n)$ and $T(n/b)$ and $T(n/b)$ and $\\ldots$ (overall $a$ times $T(n/b)$). But now $T(n/b)$ can be computed in the same way, that is, it is $f(n/b)$ plus $a$ times $T((n/b)/b)$. This results in the following steps:\n",
    "\n",
    "1. Draw the tree\n",
    "\n",
    " (a) Start with $f(n)$ at the top.\n",
    "\n",
    " (b) Draw $a$ branches. Each of these branches corresponds to one $T(n/b)$.\n",
    "\n",
    " (c) In each branch, write whatever $f(n/b)$ evaluates to. Of course, you can also use $\\ldots$, for some of these terms\n",
    " \n",
    " (d) (as in (b)) Draw $a$ branches below each of the $f(n/b)$\n",
    " \n",
    " (e) (as in (c)) Evaluate $f(n/b^2)$, and write it under the branches\n",
    " \n",
    " (f) At this point, you should have a good understanding of how the terms develop, otherwise you can draw more of the top levels. Otherwise, add some $\\ldots$ and write the $i$th level next. The terms here will correspond to $f(n/b^i)$ and you will have $a^i$ of them, which you can then simply denote by writing the term and \"# terms: $a^i$\" (or something shorter like \"$\\times a^i$\"). Here \"#\" is short for \"number of\" (or \"size of\" if you see terms as a set).\n",
    " \n",
    "(g) Also draw the last level. The last level is different, because it corresponds to the base case. So, you can write $\\Theta(1) \\ldots \\Theta(1)$ and #terms = $a^{\\log_b n}$. If the reason for this isn't clear, see more details below in the analysis.\n",
    "\n",
    "2. Analyzing the tree\n",
    "\n",
    " (a) sum up the terms per level\n",
    " \n",
    " (b) determing (and write down) the number of levels: With each application of the recurrence the \"input size\" decreases. The question is: how often do you need to apply the recurrence until you hit a base case. (Since we typically don't know the base specific base cases, you can ask, after how many applications would it be $\\leq 1$. In our specific case of $T(n) = a T(n/b) + f(n)$, this is essentially the question how often $n$ can be divided by $b$, that is, the number of levels is $\\log_b n$.\n",
    " \n",
    " (c) There are two main cases. If the contribution per level doesn't depend on the number of levels. Then calculate $T(n)$ as contribution per level times number of levels. If the contribution depends on the level, then you will need to calculate the sum of contributions. In both cases add the last level separately.\n",
    "\n",
    "\n",
    "### Recurrences: Substitution method <a class=\"anchor\" id=\"rt-substitution\"></a>\n",
    "\n",
    "1.) This is a matter of doing a proof using strong induction\n",
    "\n",
    "2.) Therefore you will need to decide on: (a) induction hypothesis, (b) choice of $n_0$, (c) base cases, and then do the proof\n",
    "\n",
    " (a) You will probably want to prove a statement of the form $T(n) = O(f(n))$. The natural induction hypothesis for this would be IH: $T(n) \\leq c \\cdot f(n)$. (Note: eventually, you don't want to work with a generic $c$, but pick a specific number $c$, for which the proof works.)But careful: This might fail because of lower order terms. There are two ways to find out which lower order terms to add (i) trial and error, and (ii) a more careful analysis of the recursion tree. \n",
    " \n",
    " Very often, trial and error already works: If you try an IH of the type $T(n) \\leq c \\cdot f(n)$, but in your proof you end up with an extra lower-order term, $g(n)$, i.e., your proof gives you T(n) \\leq c \\cdot f(n) + g(n)$, then try as IH $T(n) \\leq c \\cdot f(n) - d \\cdot g(n)$. Again, you will eventually pick a specific value for $d$. What that value is, will follow from your proof.\n",
    " \n",
    " (b) As a rule of thumb, the smaller you can choose $n_0$ the better. The reason for this is that larger $n_0$ will force you to put more effort into your base cases. A smaller $n_0$ may come at the cost of a larger $c$, but a larger $c$ commonly does not cause any difficulties. \n",
    " \n",
    " Thus, typically you would choose $n_0 = 1$. A reason not to do so, is if you have a $\\log$-term. Since $\\log(1) = 0$, in this case, you would typically end up with $n_0 = 2$.\n",
    " \n",
    " (c) Bases cases can be tricky. They mostly depend on 2 factors: Which base cases does your recursion have? Which $n_0$ did you pick? \n",
    " \n",
    " The base cases of your recursion will need to be also base cases of your induction, because you won't be able to apply the IH for these cases. The only exception is the case where $n_0$ is larger than a base case, because then you can't use that base case for your induction.\n",
    " \n",
    " If the base cases of the recursion are not given explicitly (i.e., you only have the recurrence), then assume as base cases those cases, where you would get a $T(0)$ term on the right-hand side of the recurrence. E.g., if your recurrence is $T(n) = T(n/5) + n$, then your base cases would be $n=1, n=2, \\ldots, n=4$, because $4/5$ ends up being $<1$.\n",
    " \n",
    " If $n_0 > 1$ then you might need additional base cases. Specifically, you will need those cases, which end up with a $T(m)$ with $m<n_0$ on the right-hand side of the recurrence. For instance, if we again have $T(n) = T(n/5) + n$, but we are using $n_0 = 3$, then for all $n < 15$ we cannot use the IH, therefore they need to be base cases of the induction, even if they are not base cases of the recursion. This explains why I suggest to pick $n_0$ as small as possible. \n",
    "\n",
    "### Amortized Analysis <a class=\"anchor\" id=\"rt-amortized\"></a>\n",
    "\n",
    "We want to compute $T(n)/n$, where $T(n)$ denotes the time to perform an operation $n$ time. We only use amortized analysis, if the time that an operation takes varies, but on average (over $n$ executions) is lower than the worst-case.\n",
    "\n",
    "*Simple case: The running time varies, but is known in advance*\n",
    "\n",
    "An example of this are insertions into a growable array (i.e., a Python list). While the time per insertion varies, we know for each insertion what the running time will be. More specifically, we have the case that for most insertions the running time is low, but for some it is high. It simplifies the analysis, if you handle the insertions with low running time by a bound $O(n)\\times$ the low running time. The insertions with high running time, will need to be analyzed more carefully, resulting in a sum.\n",
    "\n",
    "*Second case: The running time per operation is not known in advance, but we can bound the overall time*\n",
    "\n",
    "Amortized analysis often uses a *charging argument*. This means that there is some kind of bounded budget, and if an operation has a higher running time, it uses some part of the budget. This budget is called *potential (function)*, and the analysis method is called *potential method*.\n",
    "\n",
    "This is best illustrated by an example. Consider the following (nonsensical) insert routine:\n",
    "\n",
    "```python\n",
    "def pop_and_append(A, x):\n",
    "    if len(A) >= x: \n",
    "        for i in range(x): A.pop()\n",
    "    A.append(x)\n",
    "```\n",
    "\n",
    "This method appends $x$ to $A$, but if there are at least $x$ elements in $A$ it first pops $x$ elements. Starting with an empty array $A$, the amortized average cost of pop_and_append is $O(1)$. \n",
    "\n",
    "1. Define a budget/potential. Here the potential is len(A), the number of elements in A.\n",
    "\n",
    "2. For the budget check:  \n",
    " (a) It should be 0 at the start  \n",
    " (b) It should always be non-negative  \n",
    " (c) When does it go up? By how much does it overall increase? Here, the budget increases by 1 everytime we append, so the overall budget to spend is at most the overall number of append operations, which in this example is $n$  \n",
    " (d) Charge the varying part of the running time to the budget. Everytime iteration of the for loop takes $1$ out of the budget.\n",
    " \n",
    " Calculate $T(n)$ as base cost for operation times $n$ + overall budget. pop_and_append takes $\\Theta(1)$ time per iteration, if we exclude the for-loop. Each iteration of the for-loop takes constant time, and we can bound the number of iterations of the for-loop by the number of append-operations and therefore by $n$. Thus, $T(n) = \\Theta(1)\\cdot n + O(n) = \\Theta(n)$. Thus, the amortized average is $\\Theta(n)/n = \\Theta(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proving Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops: Loop invariant proofs <a class=\"anchor\" id=\"correctness-loops\"></a>\n",
    "\n",
    "Loop invariant proofs are covered in [this tutorial](https://www.win.tue.nl/~kbuchin/teaching/JBP030/notebooks/loop-invariants.html)\n",
    "\n",
    "Some short remarks:\n",
    "\n",
    "1. Always state a loop invariant. Without a loop invariant, a loop invariant proof doesn't make sense. \n",
    "\n",
    "2. The tutorial linked above provides help with coming up with a loop invariant\n",
    "\n",
    "3. Keep in mind that: Initialization+Maintenance together show that the loop invariant is correct. Termination makes use of the loop invariant to then prove correctness of the algorithm. For details again see the tutorial\n",
    "\n",
    "4. The value of $i$ can be easily determined from the algorithm by checking what value $i$ has at the start of the first iteration\n",
    "\n",
    "5. The value of $i$ for termination should be the first value that is no longer covered by the range for $i$ as stated in the control statement (i.e., in the first line \"for $\\ldots$\"). It might help to think of the control statement as something that explicitely increments $i$ (if it is a loop of the type \"for i in range $\\ldots$\"), and checks whether $i$ is still in the range.\n",
    "\n",
    "<details>\n",
    "  <summary>\n",
    "    $\\triangleright$ example of index at termination\n",
    "  </summary>\n",
    "  <p>\n",
    "Assume we are analyzing the algorithm to compute the maximum in an array:\n",
    "    \n",
    "```python\n",
    "def max(A):\n",
    "    answer = A[0]  \n",
    "    for j in range(1,len(A)): \n",
    "        if (A[j]>answer): answer = A[j] \n",
    "    return answer\n",
    "```\n",
    "\n",
    "To make it more explicit how the loop increments $j$, consider the equivalent while loop\n",
    "\n",
    "```python\n",
    "def max2(A):\n",
    "    answer = A[0]\n",
    "    j = 1\n",
    "    while j < len(A): \n",
    "        if (A[j]>answer): answer = A[j]\n",
    "        j = j+1\n",
    "    return answer\n",
    "```\n",
    "\n",
    "Here it is clear to see that after the last iteration we have $j = len(A)$, since this is the reason that the condition of the while statement failed. You should think of the for-loop as also incrementing $j$, until $j$ is no longer in range(1,len(A)). Then it is also clear that at termination of the loop, $j = len(A)$.\n",
    "\n",
    "  </p>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "### Recursion: Strong Induction <a class=\"anchor\" id=\"correctness-recursion\"></a>\n",
    "\n",
    "It might seem that the challenge here is to correctly apply induction. But that's actually not the case: Induction is used in a very simple way; the challenge is to get the argument within the induction step correct.\n",
    "\n",
    "1. A recursive algorithm has a base case and a recursive case. Identify both. Keep in mind that the base case could be implicit, that is, it could be that the algorithm does nothing in the base case. Typically, you can identify the base case vs recursive case by an if-statement of the form \"if $n < \\ldots$\".\n",
    "\n",
    "2. You need to prove that the algorithm works correctly in the base case. Here you can simply go through the base case(s) that might occur. This part of the proof is typically quite easy, because the algorithm probably does something fairly simple\n",
    "\n",
    "3. You need to prove that the algorithm works correctly in the recursive case (i.e., the part of the algorithm that is not the base case). Prove correctness of this case under the assumption that the recursive calls are correct. It might help to not think of the algorithm as a recursive algorithm, but as an algorithm that instead of the recursive calls, simply calls a correct algorithm for the subproblems.\n",
    "\n",
    "4. Step 2+3 prove the correctness of the algorithm. You can give the proof more structure by indeed phrasing it as a strong induction. Step 2 is the base case of the induction. Step 3 is the induction step. The induction hypothesis is that the algorithm works correctly on smaller instances/ranges. As a consequence (assuming the recursive calls are on smaller instances/ranges) by the induction hypothesis the recursive calls are correct.\n",
    "\n",
    "\n",
    "### Using basic algorithms and data structures <a class=\"anchor\" id=\"correctness-additional\"></a>\n",
    "\n",
    "There is no need to re-invent the wheel. If there is an existing algorithm or data structure you can use to solve a problem, use it. And just as you can use these algorithms and data structures, you can make use of the fact that we know and that we already have proven that they are correct. So whether you are using MergeSort, Heaps, or Depth-first search, you do not need to prove that they are correct. Thus, if you need numbers in sorted order, and you used a known sorting algorithm, say Mergesort, for sorting them, then in your correctness proof, you can state that the numbers are sorted after this sorting step by the correctness of Mergesort. That's it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Executing Algorithms\n",
    "\n",
    "In everyday life, we can often make use existing algorithms and data structures as a blackbox, and that is great. But to understand why the choice of an algorithm or data structure matters, and how they are achieve the running times that they achieve, it's good to know how they actually work. That's why we look at this in class, and that's why I ask you to reproduce certain algorithms in exercises.\n",
    "\n",
    "It is important to be able to read and execute (by hand) code. But it is also important to build intuition about how certain algorithms work. This is important for the understanding of why they work and why they work as fast as they do. In the following, we look a bit more at the intuition of how certain algorithms work. These are just pointers, and you should learn to fill in the details by also looking at the code and simply executing the algorithms by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtables: chaining <a class=\"anchor\" id=\"alg-chaining\"></a>\n",
    "\n",
    "Hashing by chaining is fairly simple, since you just need to use one hash function. There might be some confusion about where in a linked list to insert a new element. By convention, we assume that elements are inserted at the head. But since we are going to use doubly-linked lists, it doesn't really matter, whether you insert at the head or tail.\n",
    "\n",
    "### Hashtables: open addressing/probing <a class=\"anchor\" id=\"alg-probing\"></a>\n",
    "\n",
    "We have seen three ways of probing.\n",
    "\n",
    "1. Linear probing: First try to insert (likewise search) at the position that the primary hash function tells you to. If that is taken try the next, and if necessary the next and so on. If you reach the end of the table, start again at position 0. Make sure to understand why $h(k,i) = (h'(k) + i) \\mod m$ exactly gives you this. If you understand this, it also means that you have a good understanding of $h(k,i)$, which will help you with the other probing methods.\n",
    "\n",
    "2. Quadratic probing: Here I am not aware of any intuitive notion. While for linear probing $+i$ could be interpreted as probing first $h'(k)$ (+0), then the next position  ($h'(k) + 1$), and then again the next ($h'(k) + 2$), quadratic probing simply provides a formula, which you need to stick to. If you understand why linear probing can be expressed in terms of $h(k,i)$, you will also be able to simply calculate the $h(k,i)$ for quadratic probing. Of course again, the first value you test is $h'(k)$.\n",
    "\n",
    "3. Double hashing: This again is a bit (but not completely) intuitive. As in the previous cases, you first try $h'(k)$. If this fails, you compute the secondary hash function $h''(k)$ and try the cell that you get by moving this number of cells further (i.e., resulting in $h'(k) + h''(k)$), possibly wrapping around the hash table as in the case of linear probing. If that cell is unavailable (or in the case or searching: taken by a different value), again go $h''(k)$ steps further. So double hashing really is very similar to linear probing, just that instead of jumping to the next cell and then the next of the next (i.e., you always move further by 1 step), you always jump by $h''(k)$ steps.\n",
    "\n",
    "\n",
    "### Heaps: Heapify/Heapsort <a class=\"anchor\" id=\"alg-heaps\"></a>\n",
    "\n",
    "Your mantra for heap operations should be: first make sure the structure is correct, only then handle the heap property. This is true for extract-max just as well as for increase-key. In all of these cases the steps you do beyond this are very intuitive. Just make sure to apply heapify recursively. While it's intuitively obvious in extract-max, that you will move down the element that you placed at the root as far down as necessary, you might get this wrong, if you forget that heapify works recursively.\n",
    "\n",
    "About heapsort: Keep in mind that there are two parts:\n",
    "\n",
    "1. BuildHeap\n",
    "\n",
    "2. the sorting as such\n",
    "\n",
    "Don't forget the BuildHeap. This is build-heap as we have seen it in the lecture. From bottom to top (and per level: right to left, but that doesn't really matter), call MaxHeapify. Keep in mind that you can skip calling MaxHeapify on the leaves, because in this case nothing happens.\n",
    "\n",
    "The sorting step really always is the same:\n",
    " (a) swap the element at the root with the currently last element, \n",
    " (b) forget about the now last element (which was the one at the root), it is no longer part of the heap (which is now by 1 smaller),\n",
    " (c) call MaxHeapify on the rest. As above, don't forget that heapify works recursively\n",
    "\n",
    "\n",
    "### Binary search trees: Successor/Predecessor <a class=\"anchor\" id=\"alg-successor\"></a>\n",
    "\n",
    "Keep in mind that there are two cases (I will discuss Successor here, precessor is analogous):\n",
    "\n",
    "1. If there is a right child, go to it and from there as far left as possibl\n",
    "\n",
    "2. If there is no right child, go up. As long as you go up \"to the left\" the elements are smaller, so you need to continue. The first time you go to the right, you found the successor.\n",
    "\n",
    "### Graphs: BFS <a class=\"anchor\" id=\"alg-bfs\"></a>\n",
    "\n",
    "BFS is simply, but it is a good exercise to understand, when nodes are colored\n",
    "\n",
    "### Graphs: DFS/topological ordering <a class=\"anchor\" id=\"alg-dfs\"></a>\n",
    "\n",
    "For DFS, make sure to keep track of discovery and finishing times. For topological ordering, make sure that you don't stop at DFS but that you then actually write down the order.\n",
    "\n",
    "### Graphs: Dijkstra's algorithm <a class=\"anchor\" id=\"alg-dijkstra\"></a>\n",
    "\n",
    "Keep in mind: you always take from the queue the vertex/element (from the remaining elements) with the lowest distance to the source. Then you relax that vertex: For all its outgoing neighbors check whether going via that element gives you a distance smaller than the one that you computed previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental algorithms <a class=\"anchor\" id=\"design-incremental\"></a>\n",
    "\n",
    "How can you update the solution if you add new elements one by one? If this can be done, then doing so is called an incremental algorithm. And if this can be done easily (and efficiently), this approach is wort-while considering. We've seen on the slides how to design incremental algorithms. This typically is a for-loop, where in each iteration you add a new element.\n",
    "\n",
    "### Divide&Conquer <a class=\"anchor\" id=\"design-divide\"></a>\n",
    "\n",
    "Ask yourself:\n",
    "\n",
    "1.) How can I split the problem into two subproblems of equal size. If your problem has something to do with a tree, the natural split is \"left subtree\" and \"right subtree\". At the end you need to combine solutions. If we were dealing with a problem on a binary tree, make sure to also include the value at the root when combining.\n",
    "\n",
    "### Using basic algorithms and data structures <a class=\"anchor\" id=\"design-additional\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, you obtain a basic algorithmic toolkit. This includes several algorithms and data structures that are useful in many situations. Use them! \n",
    "\n",
    "If you, for instance, need to a subroutine which finds small elements quickly, think about which algorithms and data structures may help you with this: Maybe sorting, if you have enough time to sort. Or a Min-Heap if you don;t need to find too many small elements. Etc.\n",
    "\n",
    "If you can use an existing algorithm or data structure, make sure to make this clear in your code, and you certainly do not need to re-code these algorithms and data structures. An algorithm is much easier to read, if --for instance-- you say, that you are going to use MergeSort next (without talking about how to code MergeSort), rather than providing detailed pseudocode for whatever comes next, e.g., for MergeSort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
